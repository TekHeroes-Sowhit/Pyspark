{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy.get_group(name: Union[Any, Tuple[Any, …], List[Union[Any, Tuple[Any, …]]]]) → FrameLike[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf = ps.DataFrame([('falcon', 'bird', 389.0),\n",
    "                    ('parrot', 'bird', 24.0),\n",
    "                    ('lion', 'mammal', 80.5),\n",
    "                    ('monkey', 'mammal', np.nan)],\n",
    "                   columns=['name', 'class', 'max_speed'],\n",
    "                   index=[0, 2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf.GroupBy(\"class\").get_group(\"mammal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf.GroupBy(df.A).cummax().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <!-- pyspark.sql.GroupedData.sum -->\n",
    "df.groupBy().sum('age').collect()\n",
    "df3.groupBy().sum('age', 'height').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PandasCogroupedOps.applyInPandas(func: PandasCogroupedMapFunction, schema: Union[pyspark.sql.types.StructType, str]) →\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  \n",
    "def normalize(pdf):\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=(v - v.mean()) / v.std())\n",
    "df.groupby(\"id\").apply(normalize).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupedData.applyInPandas(func: PandasGroupedMapFunction, schema: Union[pyspark.sql.types.StructType, str])\n",
    "import pandas as pd  \n",
    "from pyspark.sql.functions import pandas_udf, ceil\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))  \n",
    "def normalize(pdf):\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=(v - v.mean()) / v.std())\n",
    "df.groupby(\"id\").applyInPandas(\n",
    "    normalize, schema=\"id long, v double\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row as Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row.asDict(recursive: bool = False) → Dict[str, Any\n",
    "Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n",
    "df.withColumn('a', df['a'].dropFields('b', 'c')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
    "df.select(df.r.getField(\"b\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.Column.ilike.SQL ILIKE expression (case insensitive LIKE). Returns a boolean Column based on a case insensitive match.\n",
    "df.filter(df.name.ilike('%Ice')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pyspark.sql.GroupedData.min\n",
    "# GroupedData.min(*cols: str) \n",
    "df.groupBy().min('age').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.GroupedData.mean\n",
    "df.groupBy().mean('age').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('sales').agg({'Price':'Count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\00824732\\\\Desktop\\\\geek6\\\\personal\\\\New folder (2)\\\\required'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('country').agg({'Price':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: './image.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\display.py:1027\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[1;34m(self, always_both)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1027\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1028\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\formatters.py:973\u001b[0m, in \u001b[0;36mMimeBundleFormatter.__call__\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    970\u001b[0m     method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n\u001b[0;32m    972\u001b[0m     \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 973\u001b[0m         \u001b[39mreturn\u001b[39;00m method(include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[0;32m    974\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\display.py:1017\u001b[0m, in \u001b[0;36mImage._repr_mimebundle_\u001b[1;34m(self, include, exclude)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed:\n\u001b[0;32m   1016\u001b[0m     mimetype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mimetype\n\u001b[1;32m-> 1017\u001b[0m     data, metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_and_metadata(always_both\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1018\u001b[0m     \u001b[39mif\u001b[39;00m metadata:\n\u001b[0;32m   1019\u001b[0m         metadata \u001b[39m=\u001b[39m {mimetype: metadata}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\display.py:1029\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[1;34m(self, always_both)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1028\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1029\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1030\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m md \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1032\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: No such file or directory: './image.png'"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: './image.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\display.py:1027\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[1;34m(self, always_both)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1027\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1028\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\formatters.py:343\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    341\u001b[0m     method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n\u001b[0;32m    342\u001b[0m     \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 343\u001b[0m         \u001b[39mreturn\u001b[39;00m method()\n\u001b[0;32m    344\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\display.py:1049\u001b[0m, in \u001b[0;36mImage._repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_repr_png_\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1048\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FMT_PNG:\n\u001b[1;32m-> 1049\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_and_metadata()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\display.py:1029\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[1;34m(self, always_both)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     b64_data \u001b[39m=\u001b[39m b2a_base64(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1028\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1029\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1030\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m md \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1032\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: No such file or directory: './image.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "display.Image(\"./image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- FREQUENCY TABLE OR CROSS TABLE IN PYSPARK – 2 WAY CROSS TABLE -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"country\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.crosstab('Item_group','price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Item_group','price').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.drop('country').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('custno').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['custno','city']\n",
    "df.drop(*columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "## drop multiple columns using position\n",
    " \n",
    "spark.createDataFrame(df_orders.select(df_orders.columns[:2]).take(5)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop column name which starts with the specific string  in pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql.functions import null\n",
    "def drop_null(df_orders):\n",
    "    null_counts=df_orders.select(count())\n",
    "    to_drop=[k for k,v in null_counts.items() if v>0]\n",
    "    df_orders=df_orders.drop(*to_drop)\n",
    "    return df_orders\n",
    "\n",
    "drop_null(df_orders).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop column name which contains a specific string  in pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_lis=df_orders.columns\n",
    "columns_to_drop=[i for i in some_list if i.endwith('p')]\n",
    "df_order.drop(*columns_to_drop).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics or summary statistics of dataframe in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('custno').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([mean('country_sales'),min('sales')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET DATA TYPE OF COLUMN IN PYSPARK (SINGLE & MULTIPLE COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting data type of single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_basket1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mUntitled-5.ipynb Cell 21\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-5.ipynb?jupyter-notebook#X26sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m df_basket1\u001b[39m.\u001b[39mselect(\u001b[39m'\u001b[39m\u001b[39mPrice\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mprintSchema()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_basket1' is not defined"
     ]
    }
   ],
   "source": [
    "df_basket1.select('Price').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data type of multiple column in pyspark : Method 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "df_basket1.select('Price','Item_name').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket1.select('Price','Item_name').dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data type of all the columns in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET LIST OF COLUMNS AND ITS DATA TYPE IN PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1:  using printSchema() function\n",
    "df_basket1.printSchema()\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "df_basket1.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "df_basket1.select('Price').printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data type of single column in pyspark using dtypes – Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "df_basket1.select('Price').dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MEAN, VARIANCE AND STANDARD DEVIATION OF COLUMN IN PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of the column in pyspark with example:\n",
    "df_basket1.agg({'Price': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Variance of the column in pyspark\n",
    "df_basket1.agg({'Price': 'variance'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Deviation of the column in pyspark with example:\n",
    "df_basket1.agg({'Price': 'stddev'}).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket1.groupby('Item_group').agg({'Price': 'mean'}).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_basket1.groupby('Item_group').agg({'Price': 'variance'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicate rows by keeping the Last duplicate occurrence in pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window \n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.sql.functions import row_number\n",
    " \n",
    "df_orders1 = df_orders.select(\"order_no\",\"cust_no\",\"eno\",\"received_date\",\"Shipped_date\", F.row_number().over(Window.partitionBy(\"received_date\").orderBy(df_orders['received_date'])).alias(\"row_num\")) \n",
    "df_orders1.groupBy(\"order_no\",\"cust_no\",\"eno\",\"received_date\",\"Shipped_date\").max(\"row_num\").show()\n",
    "df_orders1=df_orders.where(\"cust_no!=23512\")\n",
    "df_orders1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop NULL rows with where condition in pyspark :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df_orders1 = df_orders.where(col('Shipped_date').isNotNull())\n",
    "df_orders1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows with NA or missing values in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders1=df_orders.dropna()\n",
    "df_orders1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop rows with NA or missing values in pyspark : Method2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicate rows in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "df_orders1 = df_orders.dropDuplicates()\n",
    "df_orders1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicate rows by a specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.dropDuplicates((['cust_no'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicate rows by keeping the first duplicate occurrence in pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window \n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.sql.functions import row_number\n",
    "df_orders1 = df_orders.select(\"order_no\",\"cust_no\",\"eno\",\"received_date\",\"Shipped_date\", F.row_number().over(Window.partitionBy(\"received_date\").orderBy(df_orders['received_date'])).alias(\"row_num\")) \n",
    "df_orders1.filter(df_orders1.row_num ==1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark-select-single-multiple-columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_basket1.select(df_basket1.columns[0],df_basket1.columns[2]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select using Regex with column name like in pyspark (select column name like):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket1.select(df_basket1.colRegex(\"`(Item)+?.+`\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISTINCT ROWS OF DATAFRAME IN PYSPARK – DROP DUPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_basket.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop duplicate rows in pyspark by a specific column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket.dropDuplicates((['Price'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distinct value of a column in pyspark – distinct() \n",
    "df_basket.select(\"Item_group\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and orderby in pyspark:\n",
    "df_basket.orderBy(F.col(\"Price\").desc()).dropDuplicates().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom pyspark.sql import Window\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "tup = [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")]\n",
    "df = sqlContext.createDataFrame(tup, [\"id\", \"category\"])\n",
    "window = Window.partitionBy(\"category\").orderBy(\"id\").rowsBetween(Window.currentRow, 1)\n",
    "df.withColumn(\"sum\", func.sum(\"id\").over(window)).sort(\"id\", \"category\", \"sum\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows by keeping the first duplicate occurrence in pyspark:\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import row_number\n",
    "df_basket1 = df_basket.select(\"Item_group\",\"Item_name\",\"Price\", F.row_number().over(Window.partitionBy(\"Price\").orderBy(df_basket['price'])).alias(\"row_num\"))\n",
    "df_basket1.filter(df_basket1.row_num ==1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAISED TO POWER OF COLUMN IN PYSPARK – SQUARE, CUBE , SQUARE ROOT AND CUBE ROOT IN PYSPARK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import pow, col\n",
    " \n",
    "df.select(\"*\", pow(col(\"mathematics_score\"), 2).alias(\"Math_score_square\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cube of the column in pyspark with example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import pow, col\n",
    " \n",
    "df.select(\"*\", pow(col(\"mathematics_score\"), 3).alias(\"Math_score_cube\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power of N to the column in pyspark with example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import pow, col\n",
    " \n",
    "df.select(\"*\", pow(col(\"mathematics_score\"), 4).alias(\"Math_score_power\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Square root of the column in pyspark with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import pow, col\n",
    "df.select(\"*\", pow(col(\"mathematics_score\"), 1/2).alias(\"Math_score_squareroot\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min and Max Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket1.agg({'Price': 'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket1.agg({'Price': 'min'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round off to decimal places using round() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round, col\n",
    "df_states.select(\"*\", round(col('hindex_score'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round, col\n",
    "df_states.select(\"*\", round(col('hindex_score'),2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset-or-filter-data-with-multiple-conditions-in-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter('mathematics_score > 50 or science_score > 50').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df.filter((f.col('mathematics_score') > 50) & (f.col('science_score') > 50)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df.filter((f.col('mathematics_score') > 60)| (f.col('science_score') > 60)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.colRegex(\"`(mathe)+?.+`\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.name.endswith('i')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.name.startswith('Em')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.name.contains('an')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.functions as f\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.window import Window\n",
    "df_percent = df_basket1.withColumn('price_percent',f.col('Price')/f.sum('Price').over(Window.partitionBy())*100)\n",
    "df_percent.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum and Partition By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_percent = df_basket1.withColumn('price_percent',f.col('Price')/f.sum('Price').over(Window.partitionBy())*100)\n",
    "df_cum_percent = df_percent.withColumn('cum_percent', f.sum(df_percent.price_percent).over(Window.partitionBy().orderBy().rowsBetween(-sys.maxsize, 0)))\n",
    "df_cum_percent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "df_percent = df_basket1.withColumn('price_percent', f.col('Price')/f.sum('Price').over(Window.partitionBy('Item_group'))*100)\n",
    "df_cum_percent_grp = df_percent.withColumn('cum_percent_grp', f.sum(df_percent.price_percent).over(Window.partitionBy('Item_group').orderBy().rowsBetween(-sys.maxsize, 0)))\n",
    "df_cum_percent_grp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the dataframe in pyspark by multiple columns – descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_student_detail1 = df_student_detail.orderBy('grad_score','science_score', ascending=False)\n",
    "df_student_detail1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the dataframe in pyspark by multiple columns – ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_student_detail1 = df_student_detail.orderBy('grad_score','science_score')\n",
    "df_student_detail1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUMULATIVE SUM OF COLUMN AND GROUP IN PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as f\n",
    "cum_sum = df_basket1.withColumn('cumsum', f.sum(df_basket1.Price).over(Window.partitionBy().orderBy().rowsBetween(-sys.maxsize, 0)))\n",
    "cum_sum.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Cumulative sum of the column by Group in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rowsBetween(-sys.maxsize, 0) along with sum function is \n",
    "# used to create cumulative sum of the column, \n",
    "# an additional partitionBy() function of Item_group column \n",
    "# calculates the cumulative sum of each group as shown below\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as f\n",
    "cum_sum = df_basket1.withColumn('cumsum', f.sum(df_basket1.Price).over(Window.partitionBy('Item_group').orderBy().rowsBetween(-sys.maxsize, 0)))\n",
    "cum_sum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At First we will be replacing the missing \n",
    "# and NaN values with 0, using fill.na(0) ;\n",
    "#  then will use Sum() function and partitionBy\n",
    "#  a column name is used to calculate the cumulative \n",
    "#   sum of the “Price” column\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as f\n",
    "df_basket2=df_basket2.fillna(0)\n",
    "cum_sum = df_basket2.withColumn('cumsum', f.sum(df_basket2.Price).over(Window.partitionBy().orderBy().rowsBetween(-sys.maxsize, 0)))\n",
    "cum_sum.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get distinct value of multiple columns in pyspark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinct value of all the columns in pyspark using – distinct() function : Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get distinct value of a column in pyspark – distinct() – Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket.select(\"Price\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get distinct value of a column – dropDuplicates() – Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "df_basket.dropDuplicates((['Price'])).select(\"Price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinct Value of multiple columns in pyspark: Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket.select(\"Item_group\",\"Price\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distinct value of all the columns using dropDuplicates() function : Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "df_basket.dropDuplicates().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e57b5609a0adf6ef8af7ad6d2063e8e9c24ef6935f7306ae9ba467b68a2bc1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
